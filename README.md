# Iterative Layer-wise Distillation 
**Realisation for a distillation approach of Large Language Models (LLM)**

## Table of Contents 

## Description 
Iterative Layer-wise Distillation is a approach for distillation of Large Language Models emerged from idea presented in [ShortGPT](https://arxiv.org/pdf/2403.03853). Layer's importance is evaluated on 7 selected datasets from [LLMTF](https://github.com/RefalMachine/llmtf_open) benchmark:

* MMLU Tasks:
    * [nlpcoreteam/enMMLU](https://huggingface.co/datasets/NLPCoreTeam/mmlu_ru)
    * [nlpcoreteam/ruMMLU](https://huggingface.co/datasets/NLPCoreTeam/mmlu_ru)
* Abstractive Summarization: 
    * [treewayabstractive](https://huggingface.co/datasets/dichspace/daru_treeway_eval)
* Text copy: 
    * [cp_doc_ru](https://huggingface.co/datasets/RefalMachine/darumeru/viewer/cp_doc_ru)
    * [cp_para_ru](https://huggingface.co/datasets/RefalMachine/darumeru/viewer/cp_para_ru)
* Translation:
    * [flores_en_ru](https://huggingface.co/datasets/RefalMachine/darumeru/viewer/flores?views%5B%5D=flores_test)
    * [flores_ru_en](https://huggingface.co/datasets/RefalMachine/darumeru/viewer/flores?views%5B%5D=flores_test)


